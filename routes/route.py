<<<<<<< Updated upstream
from fastapi import APIRouter, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
import psycopg2
from datetime import datetime
from parser_prototype import preprocess_text, clean_and_convert, extract_fields, extract_line_items, parse_invoice, parse_pdf_bytes
import os
from pathlib import Path
from dotenv import load_dotenv
from typing import Any, Dict
import pdfplumber, re, json
from io import BytesIO
import hashlib
import logging
from audit_logger_v1 import validate_log_entry, audit_log
import uuid



env_path = Path('.') / '.env.example'

load_dotenv(dotenv_path=env_path)


routes = APIRouter()

@routes.get("/health")
async def health():
    return {"status": "OK"}


@routes.get("/ready")
async def ready():
    try:
        
        conn = psycopg2.connect(
            host=os.getenv("POSTGRES_HOST"),
            port=os.getenv("POSTGRES_PORT"),
            dbname=os.getenv("POSTGRES_DB"),
            user=os.getenv("POSTGRES_USER"),
            password=os.getenv("POSTGRES_PASSWORD")
        )
        cur = conn.cursor()
        cur.execute("SELECT 1;")  # simple query
        result = cur.fetchone()
        cur.close()
        conn.close()
        if result == (1,):
            return {"status": "DB connection OK"}
        else:
            return {"status": "DB connection failed"}
    except Exception as e:
        return {"status": f"DB connection failed", "error": str(e)}
    


DB_HOST = os.getenv("POSTGRES_HOST")
DB_PORT = os.getenv("POSTGRES_PORT")
DB_NAME = os.getenv("POSTGRES_DB")
DB_USER = os.getenv("POSTGRES_USER")
DB_PASS = os.getenv("POSTGRES_PASSWORD")
AUDIT_DB = os.getenv("POSTGRES_AUDIT_DB")

def get_db_connection():
    """Establishes and returns a psycopg2 database connection."""
    return psycopg2.connect(
        host=DB_HOST,
        port=DB_PORT,
        dbname=DB_NAME,
        user=DB_USER,
        password=DB_PASS
    )

def get_audit_db_connection():
    """Establishes and returns a psycopg2 database connection."""
    return psycopg2.connect(
        host=DB_HOST,
        port=DB_PORT,
        dbname=AUDIT_DB,
        user=DB_USER,
        password=DB_PASS
    )


def insert_data(data: Dict[str, Any], table: str = "parsed_data", doc_id: str = "") -> str:
    """
    Inserts data into the specified PostgreSQL table and returns the generated ID.
    
    Assumes a table structure like:
    CREATE TABLE parsed_data (
        id UUID PRIMARY KEY,
        created_at TIMESTAMP WITH TIME ZONE,
        extracted_data JSONB
    );
    """
    conn = None
    generated_id = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # SQL to insert the extracted JSON data, capture the generated UUID, and 
        # use the database's clock for the timestamp (NOW()) for consistency.
        sql = f"""
        INSERT INTO {table} (id, created_at, extracted_data)
        VALUES ({doc_id}, NOW(), %s)
        RETURNING id;
        """
        
        # Use str(data) or json.dumps(data) if you were to use the actual JSON type.
        # Since we are just inserting a single JSON object, passing the dict directly 
        # often works if psycopg2 is configured, but casting to string/JSON is safer.
        import json
        json_data = json.dumps(data)
        
        cursor.execute(sql, (json_data,))
        
        # Fetch the ID that was generated by the database
        generated_id = cursor.fetchone()[0]
        
        conn.commit()
        cursor.close()
        return str(generated_id)
        
    except psycopg2.Error as e:
        if conn:
            conn.rollback()
        print(f"Database error during insert: {e}")
        # Re-raise the exception to be caught by the FastAPI route's handler
        raise
        
    finally:
        if conn:
            conn.close()




def insert_audit_log(data: Dict[str, Any], table: str = "audit_logs", doc_id: str="") -> str:
    """
    Inserts data into the specified PostgreSQL table and returns the generated ID.
    
    Assumes a table structure like:
    CREATE TABLE audit_logs (
        id SERIAL PRIMARY KEY,
        timestamp TIMESTAMP WITH TIME ZONE NOT NULL,  -- time of action (ISO format)
        doc_id TEXT NOT NULL,                         -- file name
        run_id TEXT NOT NULL,                         -- identifier for run instance
        role TEXT CHECK (role IN ('api', 'parser', 'validator', 'benchmark', 'labeler')) NOT NULL,
        actor TEXT NOT NULL,                          -- shortened actor name (e.g., m_oh)
        action TEXT NOT NULL,                         -- action performed
        field TEXT,                                   -- JSON field where action occurred (nullable)
        "from" TEXT,                                  -- original value (nullable)
        "to" TEXT,                                    -- new value (nullable)
        status TEXT CHECK (status IN ('success', 'fail', 'corrected', 'approved', 'skipped')) NOT NULL,
        schema_version TEXT DEFAULT 'invoice_v1_reset' NOT NULL,  -- schema version
        meta JSONB,                                   -- JSON metadata (e.g., parser_version)
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );
    """
    if validate_log_entry(data):
        conn = None
        generated_id = None
        try:
            conn = get_audit_db_connection()
            cursor = conn.cursor()
            
            # SQL to insert the extracted JSON data, capture the generated UUID, and 
            # use the database's clock for the timestamp (NOW()) for consistency.
            filename = data["file_metadata"]["original_filename"]
            sql = f"""
            INSERT INTO {table} (
                timestamp,
                doc_id,
                run_id,
                role,
                actor,
                action,
                field,
                "from",
                "to",
                status,
                schema_version,
                meta
            )
            VALUES (
                NOW(),  -- timestamp
                {filename},  -- filename
                {doc_id},  -- run_id
                parser,  -- role
                null,  -- actor
                parser,  -- action
                parser,  -- field
                parse_api,  -- from
                parsed_data_db,  -- to
                success,  -- status
                COALESCE(%s, 'invoice_v1_reset'),  -- schema_version (defaults if None)
                %s   -- meta (JSON)
            )
            RETURNING id;
        """
            
            # Use str(data) or json.dumps(data) if you were to use the actual JSON type.
            # Since we are just inserting a single JSON object, passing the dict directly 
            # often works if psycopg2 is configured, but casting to string/JSON is safer.
            import json
            json_data = json.dumps(data)
            
            cursor.execute(sql, (json_data,))
            
            # Fetch the ID that was generated by the database
            generated_id = cursor.fetchone()[0]
            
            conn.commit()
            cursor.close()
            return str(generated_id)
            
        except psycopg2.Error as e:
            if conn:
                conn.rollback()
            print(f"Database error during insert: {e}")
            # Re-raise the exception to be caught by the FastAPI route's handler
            raise
            
        finally:
            if conn:
                conn.close()
    else:
        print("invalid log entry")


def get_data_by_id(record_id: str) -> dict | None:
    """
    Fetches a single record from the database by its primary key.

    Args:
        record_id: The unique identifier for the record to retrieve.

    Returns:
        A dictionary representing the database row if found, otherwise None.
    """
    conn = None
    try:
        # Establish a connection to the database
        conn = get_db_connection()
        
        # Use a 'with' statement for the cursor to ensure it's closed automatically
        with conn.cursor() as cursor:
            # IMPORTANT: Use parameterized queries to prevent SQL injection.
            # The '%s' is a placeholder, not a Python string format.
            query = f"SELECT * FROM parsed_data WHERE id = %s;"
            cursor.execute(query, (record_id,))
            
            # Fetch one result
            record = cursor.fetchone()
            
            if not record:
                return None # No record was found with that ID

            # Get column names from the cursor description
            column_names = [desc[0] for desc in cursor.description]
            
            # Combine column names with the record's values to create a dictionary
            return dict(zip(column_names, record))

    except psycopg2.Error as e:
        print(f"Database error in get_data_by_id: {e}")
        # Re-raise the exception or handle it as needed; here we return None
        return None
    finally:
        # Ensure the connection is closed even if an error occurs
        if conn:
            conn.close()


    
@routes.post("/v1/parse")
async def parse(file: UploadFile = File(...)):
    try: 
        filename = file.filename
        content_type = file.content_type
        contents = await file.read()
        file_hash = hashlib.sha256(contents).hexdigest()

        # uses parser*
        data = parse_pdf_bytes(contents)

        size = len(contents)
        # change after getting parser
        
        invoice_number = data.get("invoice_number") 
        #patient_id = 0
        subtotal_amount = data.get("subtotal_amount")
        invoice_date = data.get("invoice_date")
        total_amount = data.get("total_amount")
        line_items = data.get("line_items")

        due_date = data.get("due_date")
        patient_name = data.get("patient_name")
        patient_age = data.get("patient_age")
        patient_address = data.get("patient_address")
        patient_phone = 0
        patient_email = 0
        admission_date = data.get("admission_date")
        discharge_date = data.get("discharge_date")
        discount_amount = data.get("discount_amount")
        bed_no = 0
        provider_name = 0
        provider_email = 0
        provider_website = 0
        account_no = 0
        hospital_no = 0
        bed_no = 0
        consultant = 0
        billed_to_address = 0
        tax_percent = data.get("tax_percent")
        tax_amount = 0
        currency = 0
        payment_instructions = 0
        disclaimer = 0
        total_amount = data.get("total_amount")



        extracted_data = {
            "invoice_number": invoice_number,
            "subtotal_amount": subtotal_amount,
            "invoice_date": invoice_date,
            "total_amount": total_amount,
            "line_items": line_items,
            "patient_name": patient_name,
            "patient_age": patient_age,
            "patient_address": patient_address,
            "admission_date": admission_date,
            "discharge_date": discharge_date,
            "discount_amount": discount_amount,
            "due_date": due_date,
            "file_metadata": {
                "original_filename": filename,
                "file_size_bytes": size,
                "content_type": content_type,
                "hash": file_hash
            }
        }

        if (invoice_number is None and
        subtotal_amount is None and
        invoice_date is None and
        total_amount is None and
        line_items is None and
        patient_name is None and
        patient_age is None and
        patient_address is None and
        admission_date is None and
        discharge_date is None and
        discount_amount is None and
        due_date is None):
            return JSONResponse(
                status_code=422,
                content={
                    "status" : "failed to extract all required fields", 
                    "error": 422
                }
            )
        else:
            random_id = uuid.uuid4()
            inserted_id = insert_data(data=extracted_data, doc_id=random_id)
            insert_audit_log(data=extracted_data, doc_id=random_id)
            # Return Success Response ---
            return {
                "status": "success",
                "message": "File parsed and data stored successfully.",
                "db_id": inserted_id,
                "extracted_data": extracted_data # Return the data to confirm storage
            }
        
    except psycopg2.Error as e:
        # Handle PostgreSQL specific errors
        print(f"PostgreSQL Error: {e}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": "A database error occurred during storage."}
        )
    except Exception as e:
        # Handle general errors (file read error, JSON parsing error, etc.)
        print(f"General Error: {e}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": f"An unexpected error occurred: {str(e)}"}
        )


@routes.get("/v1/runs/{run_id}")
async def get_parsed_data(run_id: str):
    """
    Retrieves parsed invoice data by its unique database ID.
    """
    try:
        # 1. Fetch the complete record from your database
        db_record = get_data_by_id(run_id)

        if not db_record:
            raise HTTPException(status_code=404, detail="Record not found")

        # 2. Separate the metadata from the main parser data
        metadata = db_record.pop("file_metadata", {}) # Use .pop() to remove it from the main dict

        # 3. Structure the response exactly as requested
        response_data = {
            "data": db_record # The rest of the record is the parser JSON
        }
        
        return response_data

    except Exception as e:
        print(f"Error fetching record {run_id}: {e}")
        # Avoid leaking internal error details in the response
=======
from fastapi import APIRouter, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
import psycopg2
from datetime import datetime
from parser_prototype import preprocess_text, clean_and_convert, extract_fields, extract_line_items, parse_invoice, parse_pdf_bytes
import os
from pathlib import Path
from dotenv import load_dotenv
from typing import Any, Dict
import pdfplumber, re, json
from io import BytesIO
import hashlib
import logging

env_path = Path('.') / '.env.example'

load_dotenv(dotenv_path=env_path)


routes = APIRouter()

@routes.get("/health")
async def health():
    return {"status": "OK"}


@routes.get("/ready")
async def ready():
    try:
        
        conn = psycopg2.connect(
            host=os.getenv("POSTGRES_HOST"),
            port=os.getenv("POSTGRES_PORT"),
            dbname=os.getenv("POSTGRES_DB"),
            user=os.getenv("POSTGRES_USER"),
            password=os.getenv("POSTGRES_PASSWORD")
        )
        cur = conn.cursor()
        cur.execute("SELECT 1;")  # simple query
        result = cur.fetchone()
        cur.close()
        conn.close()
        if result == (1,):
            return {"status": "DB connection OK"}
        else:
            return {"status": "DB connection failed"}
    except Exception as e:
        return {"status": f"DB connection failed", "error": str(e)}
    


DB_HOST = os.getenv("POSTGRES_HOST")
DB_PORT = os.getenv("POSTGRES_PORT")
DB_NAME = os.getenv("POSTGRES_DB")
DB_USER = os.getenv("POSTGRES_USER")
DB_PASS = os.getenv("POSTGRES_PASSWORD")

def get_db_connection():
    """Establishes and returns a psycopg2 database connection."""
    return psycopg2.connect(
        host=DB_HOST,
        port=DB_PORT,
        dbname=DB_NAME,
        user=DB_USER,
        password=DB_PASS
    )


def insert_data(data: Dict[str, Any], table: str = "parsed_data") -> str:
    """
    Inserts data into the specified PostgreSQL table and returns the generated ID.
    
    Assumes a table structure like:
    CREATE TABLE parsed_data (
        id UUID PRIMARY KEY,
        created_at TIMESTAMP WITH TIME ZONE,
        extracted_data JSONB
    );
    """
    conn = None
    generated_id = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # SQL to insert the extracted JSON data, capture the generated UUID, and 
        # use the database's clock for the timestamp (NOW()) for consistency.
        sql = f"""
        INSERT INTO {table} (id, created_at, extracted_data)
        VALUES (gen_random_uuid(), NOW(), %s)
        RETURNING id;
        """
        
        # Use str(data) or json.dumps(data) if you were to use the actual JSON type.
        # Since we are just inserting a single JSON object, passing the dict directly 
        # often works if psycopg2 is configured, but casting to string/JSON is safer.
        import json
        json_data = json.dumps(data)
        
        cursor.execute(sql, (json_data,))
        
        # Fetch the ID that was generated by the database
        generated_id = cursor.fetchone()[0]
        
        conn.commit()
        cursor.close()
        return str(generated_id)
        
    except psycopg2.Error as e:
        if conn:
            conn.rollback()
        print(f"Database error during insert: {e}")
        # Re-raise the exception to be caught by the FastAPI route's handler
        raise
        
    finally:
        if conn:
            conn.close()


def get_data_by_id(record_id: str) -> dict | None:
    """
    Fetches a single record from the database by its primary key.

    Args:
        record_id: The unique identifier for the record to retrieve.

    Returns:
        A dictionary representing the database row if found, otherwise None.
    """
    conn = None
    try:
        # Establish a connection to the database
        conn = get_db_connection()
        
        # Use a 'with' statement for the cursor to ensure it's closed automatically
        with conn.cursor() as cursor:
            # IMPORTANT: Use parameterized queries to prevent SQL injection.
            # The '%s' is a placeholder, not a Python string format.
            query = f"SELECT * FROM parsed_data WHERE id = %s;"
            cursor.execute(query, (record_id,))
            
            # Fetch one result
            record = cursor.fetchone()
            
            if not record:
                return None # No record was found with that ID

            # Get column names from the cursor description
            column_names = [desc[0] for desc in cursor.description]
            
            # Combine column names with the record's values to create a dictionary
            return dict(zip(column_names, record))

    except psycopg2.Error as e:
        print(f"Database error in get_data_by_id: {e}")
        # Re-raise the exception or handle it as needed; here we return None
        return None
    finally:
        # Ensure the connection is closed even if an error occurs
        if conn:
            conn.close()


    
@routes.post("/v1/parse")
async def parse(file: UploadFile = File(...)):
    try: 
        filename = file.filename
        content_type = file.content_type
        contents = await file.read()
        file_hash = hashlib.sha256(contents).hexdigest()


        if "consent" in filename.lower():
            pass

        elif "invoice" in filename.lower(): 

        # uses parser*
            data = parse_pdf_bytes(contents)

            size = len(contents)
            # change after getting parser
            
            invoice_number = data.get("invoice_number") 
            #patient_id = 0
            subtotal_amount = data.get("subtotal_amount")
            invoice_date = data.get("invoice_date")
            total_amount = data.get("total_amount")
            line_items = data.get("line_items")

            due_date = data.get("due_date")
            patient_name = data.get("patient_name")
            patient_age = data.get("patient_age")
            patient_address = data.get("patient_address")
            patient_phone = 0
            patient_email = 0
            admission_date = data.get("admission_date")
            discharge_date = data.get("discharge_date")
            discount_amount = data.get("discount_amount")
            bed_no = 0
            provider_name = 0
            provider_email = 0
            provider_website = 0
            account_no = 0
            hospital_no = 0
            bed_no = 0
            consultant = 0
            billed_to_address = 0
            tax_percent = data.get("tax_percent")
            tax_amount = 0
            currency = 0
            payment_instructions = 0
            disclaimer = 0
            total_amount = data.get("total_amount")



            extracted_data = {
                "invoice_number": invoice_number,
                "subtotal_amount": subtotal_amount,
                "invoice_date": invoice_date,
                "total_amount": total_amount,
                "line_items": line_items,
                "patient_name": patient_name,
                "patient_age": patient_age,
                "patient_address": patient_address,
                "admission_date": admission_date,
                "discharge_date": discharge_date,
                "discount_amount": discount_amount,
                "due_date": due_date,
                "file_metadata": {
                    "original_filename": filename,
                    "file_size_bytes": size,
                    "content_type": content_type,
                    "hash": file_hash
                }
            }

            if (invoice_number is None and
            subtotal_amount is None and
            invoice_date is None and
            total_amount is None and
            line_items is None and
            patient_name is None and
            patient_age is None and
            patient_address is None and
            admission_date is None and
            discharge_date is None and
            discount_amount is None and
            due_date is None):
                return JSONResponse(
                    status_code=422,
                    content={
                        "status" : "failed to extract all required fields", 
                        "error": 422
                    }
                )
            else:
                inserted_id = insert_data(data=extracted_data)
                
                # Return Success Response ---
                return {
                    "status": "success",
                    "message": "File parsed and data stored successfully.",
                    "db_id": inserted_id,
                    "extracted_data": extracted_data # Return the data to confirm storage
                }
        
    except psycopg2.Error as e:
        # Handle PostgreSQL specific errors
        print(f"PostgreSQL Error: {e}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": "A database error occurred during storage."}
        )
    except Exception as e:
        # Handle general errors (file read error, JSON parsing error, etc.)
        print(f"General Error: {e}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": f"An unexpected error occurred: {str(e)}"}
        )


@routes.get("/v1/runs/{run_id}")
async def get_parsed_data(run_id: str):
    """
    Retrieves parsed invoice data by its unique database ID.
    """
    try:
        # 1. Fetch the complete record from your database
        db_record = get_data_by_id(run_id)

        if not db_record:
            raise HTTPException(status_code=404, detail="Record not found")

        # 2. Separate the metadata from the main parser data
        metadata = db_record.pop("file_metadata", {}) # Use .pop() to remove it from the main dict

        # 3. Structure the response exactly as requested
        response_data = {
            "data": db_record # The rest of the record is the parser JSON
        }
        
        return response_data

    except Exception as e:
        print(f"Error fetching record {run_id}: {e}")
        # Avoid leaking internal error details in the response
>>>>>>> Stashed changes
        raise HTTPException(status_code=500, detail="An error occurred while retrieving the record.")