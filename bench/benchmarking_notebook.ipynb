{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Benchmarking / Metrics Calculation\n",
        "\n",
        "**Task**: Compare parser JSONs against ground truth labels to compute precision, recall, F1-score per field.\n",
        "\n",
        "**Requirements**:\n",
        "- Load ground truth labels\n",
        "- Load parser JSONs\n",
        "- Compare each field across documents (aligned with invoice_schema_v1_reset.json)\n",
        "- Compute precision, recall, F1-score per field\n",
        "- Identify fields with frequent misses\n",
        "- Track template-level coverage (ensure all 3 templates are represented)\n",
        "- Save results in versioned output (benchmark_v0.3.json or .csv)\n",
        "\n",
        "**Schema Alignment**: This benchmark evaluates all fields from the invoice schema including required fields (invoice_number, due_date, patient_name, subtotal_amount, invoice_date, total_amount, line_items) and optional fields (patient_id, patient_age, patient_address, patient_phone, patient_email, admission_date, discharge_date, discount_amount, provider_name, bed_id).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Imports ran successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"version\": \"v0.3\",\n",
        "    \"ground_truth_csv\": \"bench/data/ground_truth/invoice_fields.csv\",\n",
        "    \"minna_parser_dir\": \"path/to/minna/parser/jsons\",  # TODO: Update when available\n",
        "    \"output_dir\": \"bench/outputs\",\n",
        "    \"test_documents\": [\n",
        "        \"invoice_T1_gen1.pdf\",  # Template 1\n",
        "        \"invoice_T2_gen1.pdf\",  # Template 2  \n",
        "        \"invoice_T3_gen1.pdf\"   # Template 3\n",
        "    ],\n",
        "    \"fields_to_evaluate\": [\n",
        "        # Required fields from schema\n",
        "        \"invoice_number\",\n",
        "        \"due_date\", \n",
        "        \"patient_name\",\n",
        "        \"subtotal_amount\",\n",
        "        \"invoice_date\",\n",
        "        \"total_amount\",\n",
        "        \"line_items\",\n",
        "        \n",
        "        # Optional fields from schema\n",
        "        \"patient_id\",\n",
        "        \"patient_age\",\n",
        "        \"patient_address\", \n",
        "        \"patient_phone\",\n",
        "        \"patient_email\",\n",
        "        \"admission_date\",\n",
        "        \"discharge_date\",\n",
        "        \"discount_amount\",\n",
        "        \"provider_name\",\n",
        "        \"bed_id\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(f\"Configuration loaded - Version: {CONFIG['version']}\")\n",
        "print(f\"Fields to evaluate: {len(CONFIG['fields_to_evaluate'])} fields\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Ground Truth Labels (Dean & Matthew D.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ground truth CSV\n",
        "gt_df = pd.read_csv(CONFIG['ground_truth_csv'])\n",
        "print(f\"Loaded ground truth for {len(gt_df)} documents\")\n",
        "print(f\"Columns: {list(gt_df.columns)}\")\n",
        "\n",
        "# Convert to dictionary for easy lookup\n",
        "ground_truth = {}\n",
        "for _, row in gt_df.iterrows():\n",
        "    filename = row['filename']\n",
        "    ground_truth[filename] = row.to_dict()\n",
        "    \n",
        "    # Parse line_items JSON string if present\n",
        "    if 'line_items' in row and pd.notna(row['line_items']):\n",
        "        try:\n",
        "            ground_truth[filename]['line_items'] = json.loads(row['line_items'])\n",
        "        except (json.JSONDecodeError, TypeError):\n",
        "            ground_truth[filename]['line_items'] = []\n",
        "\n",
        "print(f\"Ground truth loaded for documents: {list(ground_truth.keys())}\")\n",
        "\n",
        "# Display sample ground truth\n",
        "sample_doc = list(ground_truth.keys())[0]\n",
        "print(f\"\\nSample ground truth for {sample_doc}:\")\n",
        "for field in CONFIG['fields_to_evaluate']:\n",
        "    if field in ground_truth[sample_doc]:\n",
        "        print(f\"  {field}: {ground_truth[sample_doc][field]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Parser JSONs\n",
        "\n",
        "**TODO**: Update the path and loading logic when parser JSONs are available\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Replace this section when Minna's parser JSONs are available\n",
        "# Expected structure: JSON files with same naming as PDFs but .json extension\n",
        "\n",
        "minna_parser_results = {}\n",
        "\n",
        "# Placeholder: Create dummy parser results aligned with schema\n",
        "print(\"Using placeholder parser results - Replace with actual JSONs\")\n",
        "\n",
        "for doc_name in CONFIG['test_documents']:\n",
        "    # TODO: Replace with actual loading logic:\n",
        "    # json_path = Path(CONFIG['minna_parser_dir']) / f\"{Path(doc_name).stem}.json\"\n",
        "    # with open(json_path, 'r') as f:\n",
        "    #     parser_results[doc_name] = json.load(f)\n",
        "    \n",
        "    # Placeholder data aligned with invoice_schema_v1_reset.json\n",
        "    parser_results[doc_name] = {\n",
        "        # Required fields\n",
        "        \"invoice_number\": \"PLACEHOLDER_INV\",\n",
        "        \"due_date\": \"2020-02-01\",\n",
        "        \"patient_name\": \"John Doe\",\n",
        "        \"subtotal_amount\": 100.00,\n",
        "        \"invoice_date\": \"2020-01-01\",\n",
        "        \"total_amount\": 120.00,\n",
        "        \"line_items\": [\n",
        "            {\n",
        "                \"description\": \"Placeholder Service\",\n",
        "                \"code\": \"PLACEHOLDER\",\n",
        "                \"amount\": 100.00\n",
        "            }\n",
        "        ],\n",
        "        \n",
        "        # Optional fields\n",
        "        \"patient_id\": \"PLACEHOLDER_MRN\",\n",
        "        \"patient_age\": 35,\n",
        "        \"patient_address\": \"123 Main St, City, State 12345\",\n",
        "        \"patient_phone\": \"+1-555-0123\",\n",
        "        \"patient_email\": \"john.doe@email.com\",\n",
        "        \"admission_date\": \"2020-01-01\",\n",
        "        \"discharge_date\": \"2020-01-02\",\n",
        "        \"discount_amount\": 0.00,\n",
        "        \"provider_name\": \"Dr. Jane Smith\",\n",
        "        \"bed_id\": \"BED001\"\n",
        "    }\n",
        "\n",
        "print(f\"Parser results loaded for {len(parser_results)} documents\")\n",
        "print(f\"Documents: {list(parser_results.keys())}\")\n",
        "\n",
        "# Display sample parser result\n",
        "sample_doc = list(parser_results.keys())[0]\n",
        "print(f\"\\nSample parser result for {sample_doc}:\")\n",
        "for field in CONFIG['fields_to_evaluate']:\n",
        "    if field in parser_results[sample_doc]:\n",
        "        print(f\"  {field}: {parser_results[sample_doc][field]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Field Comparison Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_value(value):\n",
        "    \"\"\"Normalize values for comparison\"\"\"\n",
        "    if pd.isna(value) or value is None:\n",
        "        return None\n",
        "    if isinstance(value, str):\n",
        "        return value.strip().lower()\n",
        "    return str(value).strip().lower()\n",
        "\n",
        "def compare_scalar_field(gt_value, parser_value, field_name):\n",
        "    \"\"\"Compare scalar fields (invoice_number, patient_id, etc.)\"\"\"\n",
        "    gt_norm = normalize_value(gt_value)\n",
        "    parser_norm = normalize_value(parser_value)\n",
        "    \n",
        "    # Handle missing values\n",
        "    if gt_norm is None and parser_norm is None:\n",
        "        return True, \"both_missing\"\n",
        "    if gt_norm is None:\n",
        "        return False, \"gt_missing\"\n",
        "    if parser_norm is None:\n",
        "        return False, \"parser_missing\"\n",
        "    \n",
        "    # Exact match\n",
        "    if gt_norm == parser_norm:\n",
        "        return True, \"exact_match\"\n",
        "    \n",
        "    # Special handling for numeric fields\n",
        "    numeric_fields = ['subtotal_amount', 'total_amount', 'discount_amount', 'patient_age']\n",
        "    if field_name in numeric_fields:\n",
        "        try:\n",
        "            # Handle both string and numeric inputs\n",
        "            gt_num = float(str(gt_norm).replace('$', '').replace(',', ''))\n",
        "            parser_num = float(str(parser_norm).replace('$', '').replace(',', ''))\n",
        "            # Allow small tolerance for floating point\n",
        "            if abs(gt_num - parser_num) < 0.01:\n",
        "                return True, \"numeric_match\"\n",
        "        except (ValueError, AttributeError):\n",
        "            pass\n",
        "    \n",
        "    # Special handling for date fields\n",
        "    date_fields = ['invoice_date', 'due_date', 'admission_date', 'discharge_date']\n",
        "    if field_name in date_fields:\n",
        "        try:\n",
        "            # Normalize date formats (basic comparison)\n",
        "            gt_date = str(gt_norm).replace('-', '').replace('/', '')\n",
        "            parser_date = str(parser_norm).replace('-', '').replace('/', '')\n",
        "            if gt_date == parser_date:\n",
        "                return True, \"date_match\"\n",
        "        except (ValueError, AttributeError):\n",
        "            pass\n",
        "    \n",
        "    return False, \"mismatch\"\n",
        "\n",
        "def compare_line_items(gt_items, parser_items):\n",
        "    \"\"\"Compare line items arrays\"\"\"\n",
        "    if not gt_items and not parser_items:\n",
        "        return True, \"both_empty\"\n",
        "    if not gt_items:\n",
        "        return False, \"gt_empty\"\n",
        "    if not parser_items:\n",
        "        return False, \"parser_empty\"\n",
        "    \n",
        "    # Convert to comparable format\n",
        "    gt_normalized = []\n",
        "    for item in gt_items:\n",
        "        if isinstance(item, dict):\n",
        "            # Handle numeric amount properly\n",
        "            amount = item.get('amount', '')\n",
        "            try:\n",
        "                amount = float(str(amount).replace('$', '').replace(',', ''))\n",
        "            except (ValueError, TypeError):\n",
        "                amount = 0.0\n",
        "                \n",
        "            gt_normalized.append({\n",
        "                'code': normalize_value(item.get('code', '')),\n",
        "                'description': normalize_value(item.get('description', '')),\n",
        "                'amount': amount\n",
        "            })\n",
        "    \n",
        "    parser_normalized = []\n",
        "    for item in parser_items:\n",
        "        if isinstance(item, dict):\n",
        "            # Handle numeric amount properly\n",
        "            amount = item.get('amount', '')\n",
        "            try:\n",
        "                amount = float(str(amount).replace('$', '').replace(',', ''))\n",
        "            except (ValueError, TypeError):\n",
        "                amount = 0.0\n",
        "                \n",
        "            parser_normalized.append({\n",
        "                'code': normalize_value(item.get('code', '')),\n",
        "                'description': normalize_value(item.get('description', '')),\n",
        "                'amount': amount\n",
        "            })\n",
        "    \n",
        "    # Compare line items with tolerance for amounts\n",
        "    if len(gt_normalized) == len(parser_normalized):\n",
        "        matches = 0\n",
        "        for gt_item in gt_normalized:\n",
        "            for parser_item in parser_normalized:\n",
        "                # Compare description and amount (with tolerance)\n",
        "                desc_match = gt_item['description'] == parser_item['description']\n",
        "                amount_match = abs(gt_item['amount'] - parser_item['amount']) < 0.01\n",
        "                code_match = gt_item['code'] == parser_item['code']\n",
        "                \n",
        "                if desc_match and amount_match and code_match:\n",
        "                    matches += 1\n",
        "                    break\n",
        "        \n",
        "        if matches == len(gt_normalized):\n",
        "            return True, \"exact_match\"\n",
        "        else:\n",
        "            return False, f\"partial_match_{matches}/{len(gt_normalized)}\"\n",
        "    \n",
        "    return False, f\"length_mismatch_{len(gt_normalized)}_vs_{len(parser_normalized)}\"\n",
        "\n",
        "print(\"Field comparison functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Compute Precision, Recall, F1-Score per Field\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_field_metrics(field_name, ground_truth, parser_results, documents):\n",
        "    \"\"\"Compute precision, recall, F1-score for a specific field\"\"\"\n",
        "    \n",
        "    results = {\n",
        "        'field': field_name,\n",
        "        'total_documents': len(documents),\n",
        "        'gt_present': 0,\n",
        "        'parser_present': 0,\n",
        "        'both_present': 0,\n",
        "        'matches': 0,\n",
        "        'document_results': []\n",
        "    }\n",
        "    \n",
        "    for doc_name in documents:\n",
        "        if doc_name not in ground_truth or doc_name not in parser_results:\n",
        "            continue\n",
        "            \n",
        "        gt_value = ground_truth[doc_name].get(field_name)\n",
        "        parser_value = parser_results[doc_name].get(field_name)\n",
        "        \n",
        "        # Check presence\n",
        "        gt_has_value = gt_value is not None and pd.notna(gt_value) and str(gt_value).strip() != ''\n",
        "        parser_has_value = parser_value is not None and pd.notna(parser_value) and str(parser_value).strip() != ''\n",
        "        \n",
        "        if gt_has_value:\n",
        "            results['gt_present'] += 1\n",
        "        if parser_has_value:\n",
        "            results['parser_present'] += 1\n",
        "        if gt_has_value and parser_has_value:\n",
        "            results['both_present'] += 1\n",
        "        \n",
        "        # Compare values\n",
        "        if field_name == 'line_items':\n",
        "            is_match, match_type = compare_line_items(gt_value, parser_value)\n",
        "        else:\n",
        "            is_match, match_type = compare_scalar_field(gt_value, parser_value, field_name)\n",
        "        \n",
        "        if is_match:\n",
        "            results['matches'] += 1\n",
        "        \n",
        "        results['document_results'].append({\n",
        "            'document': doc_name,\n",
        "            'gt_value': gt_value,\n",
        "            'parser_value': parser_value,\n",
        "            'gt_present': gt_has_value,\n",
        "            'parser_present': parser_has_value,\n",
        "            'match': is_match,\n",
        "            'match_type': match_type\n",
        "        })\n",
        "    \n",
        "    # Calculate metrics\n",
        "    if results['parser_present'] > 0:\n",
        "        results['precision'] = results['matches'] / results['parser_present']\n",
        "    else:\n",
        "        results['precision'] = 0.0\n",
        "    \n",
        "    if results['gt_present'] > 0:\n",
        "        results['recall'] = results['matches'] / results['gt_present']\n",
        "    else:\n",
        "        results['recall'] = 0.0\n",
        "    \n",
        "    if results['precision'] + results['recall'] > 0:\n",
        "        results['f1_score'] = 2 * (results['precision'] * results['recall']) / (results['precision'] + results['recall'])\n",
        "    else:\n",
        "        results['f1_score'] = 0.0\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Compute metrics for all fields\n",
        "field_metrics = {}\n",
        "\n",
        "for field in CONFIG['fields_to_evaluate']:\n",
        "    print(f\"Computing metrics for field: {field}\")\n",
        "    field_metrics[field] = compute_field_metrics(field, ground_truth, minna_parser_results, CONFIG['test_documents'])\n",
        "\n",
        "print(\"Field metrics computed for all fields\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Display Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary DataFrame\n",
        "summary_data = []\n",
        "for field, metrics in field_metrics.items():\n",
        "    summary_data.append({\n",
        "        'Field': field,\n",
        "        'Precision': f\"{metrics['precision']:.3f}\",\n",
        "        'Recall': f\"{metrics['recall']:.3f}\",\n",
        "        'F1-Score': f\"{metrics['f1_score']:.3f}\",\n",
        "        'GT Present': metrics['gt_present'],\n",
        "        'Parser Present': metrics['parser_present'],\n",
        "        'Matches': metrics['matches'],\n",
        "        'Total Docs': metrics['total_documents']\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"FIELD-LEVEL METRICS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# Identify fields with frequent misses\n",
        "print(\"\\nFIELDS WITH FREQUENT MISSES:\")\n",
        "print(\"=\" * 40)\n",
        "for field, metrics in field_metrics.items():\n",
        "    if metrics['f1_score'] < 0.5:  # Threshold for \"frequent misses\"\n",
        "        print(f\"{field}: F1={metrics['f1_score']:.3f} (Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f})\")\n",
        "    elif metrics['f1_score'] < 0.8:\n",
        "        print(f\"{field}: F1={metrics['f1_score']:.3f} (Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f})\")\n",
        "    else:\n",
        "        print(f\"{field}: F1={metrics['f1_score']:.3f} (Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Track Template-Level Coverage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze template coverage\n",
        "template_coverage = {\n",
        "    'T1': {'documents': [], 'avg_f1': 0.0},\n",
        "    'T2': {'documents': [], 'avg_f1': 0.0}, \n",
        "    'T3': {'documents': [], 'avg_f1': 0.0}\n",
        "}\n",
        "\n",
        "for doc_name in CONFIG['test_documents']:\n",
        "    # Extract template from filename (e.g., invoice_T1_gen1.pdf -> T1)\n",
        "    template = doc_name.split('_')[1] if '_' in doc_name else 'Unknown'\n",
        "    \n",
        "    if template in template_coverage:\n",
        "        template_coverage[template]['documents'].append(doc_name)\n",
        "        \n",
        "        # Calculate average F1 for this document across all fields\n",
        "        doc_f1_scores = []\n",
        "        for field, metrics in field_metrics.items():\n",
        "            doc_result = next((r for r in metrics['document_results'] if r['document'] == doc_name), None)\n",
        "            if doc_result:\n",
        "                # Use field-level F1 as proxy for document-level performance\n",
        "                doc_f1_scores.append(metrics['f1_score'])\n",
        "        \n",
        "        if doc_f1_scores:\n",
        "            template_coverage[template]['avg_f1'] = np.mean(doc_f1_scores)\n",
        "\n",
        "print(\"TEMPLATE-LEVEL COVERAGE\")\n",
        "print(\"=\" * 50)\n",
        "for template, data in template_coverage.items():\n",
        "    print(f\"\\nðŸ”¹ Template {template}:\")\n",
        "    print(f\"   Documents: {data['documents']}\")\n",
        "    print(f\"   Average F1: {data['avg_f1']:.3f}\")\n",
        "    \n",
        "    if len(data['documents']) == 0:\n",
        "        print(f\"   WARNING: No documents found for template {template}\")\n",
        "    elif data['avg_f1'] < 0.5:\n",
        "        print(f\"   Poor performance on template {template}\")\n",
        "    elif data['avg_f1'] < 0.8:\n",
        "        print(f\"   Moderate performance on template {template}\")\n",
        "    else:\n",
        "        print(f\"Good performance on template {template}\")\n",
        "\n",
        "# Verify all 3 templates are represented\n",
        "missing_templates = [t for t, data in template_coverage.items() if len(data['documents']) == 0]\n",
        "if missing_templates:\n",
        "    print(f\"\\nMISSING TEMPLATES: {missing_templates}\")\n",
        "    print(\"   Please ensure all 3 templates (T1, T2, T3) are represented in test documents\")\n",
        "else:\n",
        "    print(f\"\\nAll 3 templates are represented in the benchmark\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Results in Versioned Output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare comprehensive results\n",
        "benchmark_results = {\n",
        "    'metadata': {\n",
        "        'version': CONFIG['version'],\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'test_documents': CONFIG['test_documents'],\n",
        "        'fields_evaluated': CONFIG['fields_to_evaluate'],\n",
        "        'total_documents': len(CONFIG['test_documents'])\n",
        "    },\n",
        "    'field_metrics': field_metrics,\n",
        "    'template_coverage': template_coverage,\n",
        "    'summary': {\n",
        "        'overall_avg_precision': np.mean([m['precision'] for m in field_metrics.values()]),\n",
        "        'overall_avg_recall': np.mean([m['recall'] for m in field_metrics.values()]),\n",
        "        'overall_avg_f1': np.mean([m['f1_score'] for m in field_metrics.values()]),\n",
        "        'fields_with_frequent_misses': [\n",
        "            field for field, metrics in field_metrics.items() \n",
        "            if metrics['f1_score'] < 0.5\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path(CONFIG['output_dir'])\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save JSON results\n",
        "json_filename = f\"benchmark_{CONFIG['version']}.json\"\n",
        "json_path = output_dir / json_filename\n",
        "\n",
        "with open(json_path, 'w') as f:\n",
        "    json.dump(benchmark_results, f, indent=2, default=str)\n",
        "\n",
        "print(f\"JSON results saved to: {json_path}\")\n",
        "\n",
        "# Save CSV summary\n",
        "csv_filename = f\"benchmark_{CONFIG['version']}_summary.csv\"\n",
        "csv_path = output_dir / csv_filename\n",
        "\n",
        "summary_df.to_csv(csv_path, index=False)\n",
        "print(f\"CSV summary saved to: {csv_path}\")\n",
        "\n",
        "# Save detailed CSV with document-level results\n",
        "detailed_data = []\n",
        "for field, metrics in field_metrics.items():\n",
        "    for doc_result in metrics['document_results']:\n",
        "        detailed_data.append({\n",
        "            'field': field,\n",
        "            'document': doc_result['document'],\n",
        "            'gt_value': doc_result['gt_value'],\n",
        "            'parser_value': doc_result['parser_value'],\n",
        "            'match': doc_result['match'],\n",
        "            'match_type': doc_result['match_type'],\n",
        "            'field_precision': metrics['precision'],\n",
        "            'field_recall': metrics['recall'],\n",
        "            'field_f1': metrics['f1_score']\n",
        "        })\n",
        "\n",
        "detailed_df = pd.DataFrame(detailed_data)\n",
        "detailed_filename = f\"benchmark_{CONFIG['version']}_detailed.csv\"\n",
        "detailed_path = output_dir / detailed_filename\n",
        "detailed_df.to_csv(detailed_path, index=False)\n",
        "\n",
        "print(f\"Detailed CSV saved to: {detailed_path}\")\n",
        "\n",
        "print(f\"\\nBenchmark results saved successfully!\")\n",
        "print(f\"Overall Performance:\")\n",
        "print(f\"   Average Precision: {benchmark_results['summary']['overall_avg_precision']:.3f}\")\n",
        "print(f\"   Average Recall: {benchmark_results['summary']['overall_avg_recall']:.3f}\")\n",
        "print(f\"   Average F1-Score: {benchmark_results['summary']['overall_avg_f1']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Next Steps\n",
        "\n",
        "**TODO Tasks for when parser JSONs are available:**\n",
        "\n",
        "1. **Update Section 4**: Replace the placeholder parser loading logic with actual JSON file loading\n",
        "2. **Update CONFIG**: Set the correct path to `minna_parser_dir`\n",
        "3. **Verify JSON format**: Ensure the parser JSONs match the expected field structure\n",
        "4. **Re-run benchmark**: Execute all cells to get real metrics\n",
        "\n",
        "**Current Status:**\n",
        "- Ground truth loading implemented\n",
        "- Field comparison functions implemented  \n",
        "- Precision/Recall/F1 calculation implemented\n",
        "- Template coverage tracking implemented\n",
        "- Versioned output saving implemented\n",
        "- Waiting for parser JSONs to complete the benchmark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
