{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Early Benchmarking Notebook (Reproducible)\n",
        "\n",
        "**Purpose**: Minimal run with 2–3 docs via /v1/parse → capture text recall proxies, schema-pass result, and which taxonomy bucket errors fell into.\n",
        "\n",
        "**Features**:\n",
        "- Record config/seed and environment hash for re-runs\n",
        "- Capture text recall proxies\n",
        "- Schema-pass validation\n",
        "- Error taxonomy classification\n",
        "- Reproducible results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add bench directory to path\n",
        "sys.path.append('/app/bench')\n",
        "\n",
        "# Import our benchmark class\n",
        "from early_benchmark import EarlyBenchmark\n",
        "\n",
        "print(\"Imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Initialize Benchmark\n",
        "\n",
        "Create benchmark instance with default configuration:\n",
        "- **Test Documents**: 3 representative invoices\n",
        "- **Engines**: pdfplumber, pdfminer  \n",
        "- **Seed**: 42 (for reproducibility)\n",
        "- **Version**: v1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Create benchmark instance\n",
        "benchmark = EarlyBenchmark()\n",
        "\n",
        "print(f\"Run ID: {benchmark.run_id}\")\n",
        "print(f\"Environment Hash: {benchmark.environment_hash}\")\n",
        "print(f\"PDF Directory: {benchmark.pdf_dir}\")\n",
        "print(f\"Ground Truth: {benchmark.gt_csv}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Run Minimal Benchmark\n",
        "\n",
        "Execute benchmark with 2-3 documents and capture:\n",
        "- **Text Recall Proxies**: Field extraction accuracy\n",
        "- **Schema Pass Results**: Validation success/failure  \n",
        "- **Error Taxonomy Buckets**: Classification of errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Run the benchmark\n",
        "results = benchmark.run_minimal_benchmark()\n",
        "benchmark.results = results\n",
        "\n",
        "print(\"Benchmark completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Results Summary\n",
        "\n",
        "View benchmark results including:\n",
        "- Success rates by engine\n",
        "- Schema pass rates\n",
        "- Text recall proxies\n",
        "- Error taxonomy breakdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Print detailed summary\n",
        "benchmark.print_summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Save Results\n",
        "\n",
        "Save benchmark results for reproducibility and compliance tracking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Save results\n",
        "output_file = benchmark.save_results()\n",
        "\n",
        "print(f\"Results saved to: {output_file}\")\n",
        "print(f\"Run ID: {benchmark.run_id}\")\n",
        "print(f\"Environment Hash: {benchmark.environment_hash}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
